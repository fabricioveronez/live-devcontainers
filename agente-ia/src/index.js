import { config } from 'dotenv';
import { HumanMessage } from '@langchain/core/messages';
import { getChatModel } from './model/chat.js';

// Carrega as variÃ¡veis de ambiente do arquivo .env
config();

/**
 * FunÃ§Ã£o principal que demonstra interaÃ§Ã£o bÃ¡sica com o modelo de linguagem
 */
const main = async () => {
    try {
        console.log('ğŸš€ Iniciando interaÃ§Ã£o com o modelo de linguagem...\n');

        // ObtÃ©m o modelo configurado
        const model = getChatModel();

        // Define a mensagem do usuÃ¡rio
        const message = [
            new HumanMessage("O que Ã© Kubernetes?")
        ];

        console.log('ğŸ’¬ Pergunta: O que Ã© Kubernetes?\n');
        console.log('ğŸ¤– Processando resposta...\n');

        // Gera a resposta
        const response = await model.invoke(message);
        
        console.log('ğŸ“ Resposta do modelo:');
        console.log('=' .repeat(50));
        console.log(response.content);
        console.log('=' .repeat(50));

    } catch (error) {
        if (error.code === 'ECONNREFUSED' || error.message.includes('connect')) {
            console.error('âŒ Erro de conexÃ£o: NÃ£o foi possÃ­vel conectar ao servidor do modelo.');
            console.error('ğŸ”§ Verifique se o serviÃ§o estÃ¡ rodando em localhost:11434 ou model-runner.docker.internal');
            console.error(`ğŸ“‹ Detalhes do erro: ${error.message}`);
        } else {
            console.error('ğŸ’¥ Erro inesperado:');
            console.error(error.message);
            console.error('\nğŸ” Stack trace:');
            console.error(error.stack);
        }
        process.exit(1);
    }
};

// Executa a funÃ§Ã£o principal
main().catch(console.error);